{"pages":[{"title":"Curriculum Vitae","text":"Source: https://github.com/adrinjalali/cv/blob/master/adrin-jalali.pdf function renderPDF(url, canvasContainer, options) { var options = options || { scale: 1.5 }; function renderPage(page) { var viewport = page.getViewport(options.scale); var canvas = document.createElement('canvas'); var ctx = canvas.getContext('2d'); var renderContext = { canvasContext: ctx, viewport: viewport }; canvas.height = viewport.height; canvas.width = viewport.width; canvasContainer.appendChild(canvas); page.render(renderContext); } function renderPages(pdfDoc) { for(var num = 1; num <= pdfDoc.numPages; num++) pdfDoc.getPage(num).then(renderPage); } PDFJS.disableWorker = true; PDFJS.getDocument(url).then(renderPages); } renderPDF('/files/adrin-jalali.pdf', document.getElementById('holder'));","tags":"misc","url":"pages/curriculum-vitae.html"},{"title":"Get in Touch","text":"Mailing address Campus E1 4, Saarbrücken, Germany, Postal Code ( PLZ ): 66123 E-Mails ajalali &#94;as-usual&#94; mpi-inf.mpg.de (preferred for official emails) adrin.jalali &#94;as-usual&#94; gmail.com (preferred for non-official emails) Professional profiles Linkedin , github , stackoverflow , and oDesk . Social networks You can find me as adrin.jalali or adrinjalali on Twitter, G+, FB and probably some other places. But none of them are my preferred means of communication with people I don't personally know. Blogs If you are looking for an official blog of mine, you are surfing it right now. If you are curious to know what I would probably say outside work, you can check out Adrin - The Idiot , which is currently my most active blog. But I also have Mazdayasn which is mostly in my mother tongue i.e. Persian. The above two blogs are my personal thoughts and can not be cited as what I would say in an official situation. They might be far from being politically right, and I take no responsibility over them.","tags":"misc","url":"pages/get-in-touch.html"},{"title":"Home","text":"Who You are surfing Adrin Jalali ‘s page, who works as a data scientist/engineer and has spent his PhD (not finished yet) doing research in the field of Computational Biology. Area of research I use/design machine learning tools to classify samples. The datasets I work on vary from microarray to DNA methylation data. I am mostly focused on using gene/protein networks in order to help the classification task, while keeping in mind I need to interpret my results for a biologist. Therefore my goal at the moment is to have a method which directly uses or is inspired by the biological networks, classifies my samples, and can be interpreted on the biological level. [ Poster ] Adrin Jalali, Nico Pfeifer, \" Analyzing How Protein Interaction Networks Improve Classification Performance in Gene Expression Data Analysis \", ISMB / ECCB 2013, Berlin, Germany. Jalali A., and Pfeifer N., \" Interpretable per Case Weighted Ensemble Method for Cancer Associations \", BMC Genomics, volume 17, no. 1, 2016. I also worked on flow-cytometry data which is a single cell level data. I did it mostly when I was for 15 months in Vancouver, Canada, and I was doing research in British Columbia Cancer Research Center and The University of British Columbia . As a result of this part of my research, I can refer you to: Kieran O'Neill†, Adrin Jalali†, Nima Aghaeepour†, Holger Hoos, and Ryan R. Brinkman. \" Enhanced flowType/RchyOptimyx: A Bioconductor pipeline for discovery in high-dimensional cytometry data. \" Bioinformatics (2014), doi: 10.1093/bioinformatics/btt770 Nima Aghaeepour†, Adrin Jalali†, Kieran O'Neill, Pratip K. Chattopadhyay, Mario Roederer, Holger H. Hoos, and Ryan R. Brinkman. \" RchyOptimyx: Cellular hierarchy optimization for flow cytometry. \" Cytometry Part A 81, no. 12 (2012): 1022-1030, doi: 10.1002/cyto.a.22209 Nima Aghaeepour, Pratip K. Chattopadhyay, Anuradha Ganesan, Kieran O'Neill, Habil Zare, Adrin Jalali, Holger H. Hoos, Mario Roederer, and Ryan R. Brinkman. \" Early immunologic correlates of HIV protection can be identified from computational analysis of complex multivariate T-cell flow cytometry assays. \" Bioinformatics 28, no. 7 (2012): 1009-1016, doi: 10.1093/bioinformatics/bts082 [ Poster ] Adrin Jalali, Nima Aghaeepour, Kieran O'Neill, Andrew P. Weng, Ryan R. Brinkman, \" Analysis and Classification of Lymphoma Sub-types \", British Columbia Cancer Agency Annual Conference, Victoria, BC , Canada, 2012. †: equal contribution List of publications available on Google Scholar .","tags":"misc","url":"."},{"title":"Clue-WATTx Hackathon","text":"Hackathon To me, a hackathon is when a group of people gather for a day or two, working for a somewhat common goal, develop, have fun, meet new people, and to home. Unfortunately for whatever reason, most hackathons have become a competition, with a set prize, organized by some company. The good part is that some attendees might get some money out of the hackathon, but the down side is that it usually becomes a competition and people stop collaborating. I was very happy to see that was not very the case in this hackathon, and we had mostly a really nice atmosphere for the whole weekend. It was a pleasurable experience working and chatting with Clue and WATTx people as well as the attendees. Clue Clue is for people who experience menstruation cycles, or periods, to record their cycle, and related symptoms. These symptoms include bleeding, pain, skin condition, premenstrual syndrome ( PMS ), among others. The app in return, gives you predictions regarding your period, ovulation, and PMS . The app looks like this: Using the app you can easily see if there are any irregularities in your cycle, and/or see when you should be expecting phases such as ovulation, period, or PMS , and read about menstruation cycle in general if you're interested. Some symptoms can be recorded and predicted directly, such as bleeding, others are less directly observed such as ovulation. The app uses observations from other related symptoms to predict these phases. The data that the app collects is invaluable to a better understanding of the menstruation cycle, related diseases, and even (birth control) pills. I hope the data helps our society to understand cycles better, and potentially improve half of the population's lives. Statice by WATTx Fortunately, people at Clue value users' privacy a great deal. But that means they can't have a hackathon and give people real users' data to work on. Here's where the Statice platform designed by the nice folks at WATTx comes in. Their platform is still taking shape and this hackathon was also a test for them to see how the platform works. But in short, they take the real data, produce some data using the real one, which resembles the real data, but users' profiles from the real data are not revealed by looking at the synthesized data. Their synthesized data was what we worked on, and using that data we developed our method. Then to test the method against the real data, we would submit our method in the form of a docker container, and they would use that container to train our models on the real data, and then test it against the test set, again from the real data. And then, we would see the score of the method, or failed in case there was something wrong with the code or during the run. Goal The goal of the hackathon was to analyze Clue users' data given users' history, and predict users' symptoms for their next cycle. We were supposed to predict the probability of a user tracking a symptom for each day of the next cycle. Out of total 80 symptoms for which we had data, we were to predict only 16, but we could use other symptoms' history and data for the prediction. Method The main part of the data was in the form of user interactions. Basically one row for each (user, symptom, date) tuple. We transformed the data into one row per user as our input, and a vector for each symptom as output, and then used a simple regularized linear model to predict the output, i.e. Lasso. A more detailed explanation of the method as well as the code itself are available on github . Results At the end of the two days, we all submitted our best models to test against a test dataset which was taken out from the data for this purpose, and our team managed to win the prize for the best performing method: Organizers' summary A nice summary of the event was written by Laure Sorba, WATTx UX strategist, available here . And here is the diagram showing her summary of the event, available on the same page:","tags":"machine-learning","url":"clue-wattx-hackathon.html"},{"title":"TV -ad Attribution, Gaussian Processes","text":"Problem description: This work was done in Mister Spex GmbH , and slides of a presentation I gave at PyData meetup are available now here . There is a website, in this case an e-shop, and we have information about user sessions on the website. We also have information about TV -ads shown to the public requested by the company. The question is, which of those sessions on the website are there because of the TV -ads. There are some obstacles to answer the above question: Users don't tell the website why they came to the website, or how they found about the service. Users use different channels to get to the website. They might have seen an ad on TV and then have used google to find the exact address to the website. As a result, we change the question to the following: what is the likelihood of each session to be because of a TV -ad shown recently. Hence, the above input/output is given/desired: Input: One entry for each session which are date-time labeled. A list of date-time labeled events. Output: What is the likelihood of each session to be influenced by a nearby event. Prepare the data We reduce the granularity of the data and aggregate it by minute. As a result, we have session count per minute, and the time of ( TV -ad) events by minute. This results in a time series data having some points on it labeled by our events. One way to see if the events have had an influence on our traffic, is to forget about those events, detect anomalies on our time series data, and see if they've happened around those events. I've used some of those methods in other projects, and they don't make me happy on this data. They either detect wrong signals as anomalies, or miss some not so obvious signals. Before going further, let's look at the data. Fig. 1 illustrates a week worth of data. The X axis represents time (1 for each minute), and the Y axis shows the normalized session count. The data is normalized as X / median(X) simply to anonymize the actual session counts, since the actual numbers are irrelevant to this article. Fig. 1: Normalized session count for a week Some of those sharp peaks are right after a TV -ad event. Field knowledge says most of the traffic caused by a TV -ad comes up to 8 minutes after the event. I take precaution, and assume some error in reported time of the event. Now let's remove any reported session from 2 minutes before the event, to 20 minutes after the event. Fig. 2 and 3 show one day of the data, before and after removing data around reported events. Fig. 2: Normalized session count for a day Fig. 3: Normalized session count for a day, after removing data around reported events Method Intuitively speaking, I would like to remove the parts of the data close to the given events, fit a model to the remaining data, and then see how close the observed data is to the model's predictions. If the observed values are significantly higher than the predicted values, most probably some of those sessions are because of the event. Given an observed and predicted values, deciding whether or not the observed value is significantly larger, can be tricky, and in some cases very arbitrary. Using a method which gives you the probability of an observed value at a given input would make the process much easier. One way of achieving such a goal is to have a method which, intuitively speaking, tells you what it thinks the predicted value is, and also how confident it is. A Gaussian Process ( GP ) is such a model, in a way that it gives a normal distribution at a given input point as the output. This predicted output has a mean and a variance, which we use to see how probable a given observation is, under the assumption that the model correctly describes the data. Given a predicted normal distribution, the further the observed value is from the expected mean, the less expected it is. In other words, the larger the blue area shown in Fig. 4, the more probable it's an outlier. Fig. 4: Normal distribution and three different observed values - credit: thefemalecelebrity.com The area under the curve marked by the blue areas shown in Fig. 4 is between 0 and 1. It is 0 if the expected value is the same as our observed value, and is close to 1 if it is the two observed and expected mean are far from each other. Hereafter score(x) represents this area under the curve. The piece of code bellow shows how this is calculated in python: import math import scipy def phi ( x ): return 0.5 + 0.5 * scipy . special . erf ( x / math . sqrt ( 2 )) def score ( x ): return 1 - abs ( phi ( x ) - phi ( - x )) x_score = score (( x - expected_mean ) / expected_std ) Now for a moment, assume we know a given data point (number of sessions) is influenced by a TV -ad. Still not all those sessions are because of the event. Some of them would have been there even with no TV -ad event. We take the predicted mean as the background session count, and the rest as TV -ad influenced sessions. Therefore, if x is observed value, and x' is predicted value, (x - x') / x is the portion of sessions attributed to a TV -ad event. At the end, we take score(x) * (x - x') / x as the likelihood of each session being attributed to a TV -ad. Implementation and Results I used Python3 , matplotlib to generate the plots, and GPy to train the Gaussian Process model. There are at least two ways of fitting and using GPs in our setting. One is to train a really nice model using the historical data, and use the saved trained model to get predictions for future data. By looking at the data, I would sum these three kernels and optimize on the data: A periodic kernel to handle periodicity A Gaussian ( RBF ) kernel to handle the non-periodic part of the data A white noise kernel to handle fluctuations seen in the data Because there were no big enough server available, training a model on the historical data at once was not feasible. Instead, for each day, I take a small part of the data which includes 10 hours before and after start and end of the day, and train a model on that part of the data. This makes training of the model on a small machine feasible, and it also doesn't require the periodic kernel, which makes the optimization process even faster. Therefore I only use these two kernels: A Gaussian ( RBF ) kernel to handle the non-periodic part of the data A white noise kernel to handle fluctuations seen in the data To summarize, here's what I do: For a given date, take the 44 hours of data corresponding to the given date (10 hours before and 10 hours after the beginning and the end of the day respectively) Remove the part of the data around given any TV -ad events in that range. Fit a GP to the data. Calculate the abovementioned scores for each data point which corresponds to one minute. Using GPy , I train the model as: kernel = GPy.kern.RBF(input_dim=1) + GPy.kern.White(input_dim=1) m = GPy.models.GPRegression(Xtr.reshape(-1,1),ytr.reshape(-1,1),kernel) m.optimize() In the above code, Xtr and ytr are the input/output vectors respectively. The Xtr is a sequence of integers representing the minute distance from where the data started, and ytr is the normalized number of sessions for that minute. This results in a model depicted in Fig. 5. The blue gradient represents the variance of the expected output, showing the expected value should be somewhere in the blue region. Please note that we're only interested in the regions inside the data, and not the region outside the data range. Fig. 5: The trained Gaussian Process model on the 44h of data. Table 1 shows calculated scores and likelihoods of each session in each minute to be influenced by a TV -ad. A negative portion and likelihood simply mean the expected value is larger that the observed, and can safely be ignored. The Is Significant column simply flags if the score is over 0.9 . I would personally attribute only sessions of those minutes to a TV -ad. Also, as expected, not all TV -ad events result in significant rise in the traffic, and some clearly result in more traffic than others. Observed Expected Mean Expected Variance Score Portion Likelihood Is Significant TV -ad 0.96 0.84 0.03 0.52 0.12 0.06 0.96 0.84 0.03 0.52 0.12 0.06 TV 0.83 0.84 0.03 0.06 -0.01 -0.00 0.96 0.84 0.03 0.51 0.12 0.06 0.81 0.84 0.03 0.16 -0.04 -0.01 0.68 0.84 0.03 0.67 -0.24 -0.16 0.85 0.84 0.03 0.04 0.01 0.00 0.72 0.84 0.03 0.52 -0.17 -0.09 0.70 0.84 0.03 0.60 -0.20 -0.12 1.00 0.84 0.03 0.65 0.16 0.10 TV 1.89 0.84 0.03 1.00 0.55 0.55 * 1.19 0.84 0.03 0.96 0.29 0.28 * 0.94 0.85 0.03 0.41 0.10 0.04 0.85 0.85 0.03 0.02 0.01 0.00 0.96 0.85 0.03 0.49 0.12 0.06 0.83 0.85 0.03 0.08 -0.02 -0.00 0.96 0.85 0.03 0.48 0.11 0.06 1.23 0.85 0.03 0.98 0.31 0.31 * 1.11 0.85 0.03 0.87 0.23 0.20 0.85 0.85 0.03 0.01 0.00 0.00 1.04 0.85 0.03 0.74 0.18 0.14 TV 0.77 0.85 0.03 0.38 -0.11 -0.04 1.04 0.85 0.03 0.73 0.18 0.13 1.11 0.85 0.03 0.86 0.23 0.20 1.13 0.85 0.03 0.89 0.24 0.21 1.09 0.86 0.03 0.82 0.21 0.17 1.19 0.86 0.03 0.95 0.28 0.27 * 1.00 0.86 0.03 0.59 0.14 0.08 0.79 0.86 0.03 0.32 -0.09 -0.03 1.11 0.86 0.03 0.84 0.22 0.19 0.64 0.86 0.03 0.80 -0.35 -0.28 0.85 0.86 0.03 0.06 -0.01 -0.00 0.96 0.87 0.03 0.41 0.10 0.04 0.85 0.87 0.03 0.07 -0.02 -0.00 0.83 0.87 0.03 0.17 -0.05 -0.01 1.34 0.87 0.03 0.99 0.35 0.35 * 0.85 0.87 0.03 0.09 -0.02 -0.00 1.17 0.87 0.03 0.91 0.25 0.23 * 0.83 0.88 0.03 0.21 -0.05 -0.01 0.77 0.88 0.03 0.48 -0.14 -0.07 1.02 0.88 0.03 0.59 0.14 0.08 TV 1.62 0.88 0.03 1.00 0.46 0.46 * TV 1.47 0.88 0.03 1.00 0.40 0.40 * TV 1.26 0.89 0.03 0.97 0.29 0.29 * 1.19 0.89 0.03 0.92 0.26 0.23 * 1.28 0.89 0.03 0.97 0.30 0.30 * 0.91 0.89 0.03 0.11 0.03 0.00 1.13 0.89 0.03 0.82 0.21 0.17 1.15 0.90 0.03 0.86 0.22 0.19 TV 4.45 0.90 0.03 1.00 0.80 0.80 * TV 5.40 0.90 0.03 1.00 0.83 0.83 * 3.21 0.90 0.03 1.00 0.72 0.72 * 2.30 0.91 0.03 1.00 0.61 0.61 * 1.96 0.91 0.03 1.00 0.54 0.54 * 1.98 0.91 0.03 1.00 0.54 0.54 * 1.30 0.91 0.03 0.97 0.30 0.29 * 1.47 0.92 0.03 1.00 0.38 0.37 * 0.94 0.92 0.03 0.08 0.02 0.00 1.00 0.92 0.03 0.35 0.08 0.03 1.40 0.93 0.03 0.99 0.34 0.34 * 1.30 0.93 0.03 0.97 0.28 0.28 * 1.11 0.93 0.03 0.70 0.16 0.11 1.00 0.93 0.03 0.30 0.07 0.02 1.19 0.94 0.03 0.87 0.21 0.18 1.53 0.94 0.03 1.00 0.39 0.39 * 1.19 0.94 0.03 0.86 0.21 0.18 1.17 0.95 0.03 0.81 0.19 0.16 1.17 0.95 0.03 0.81 0.19 0.15 1.11 0.95 0.03 0.64 0.14 0.09 TV 1.55 0.96 0.03 1.00 0.38 0.38 * TV 2.43 0.96 0.03 1.00 0.60 0.60 * 1.51 0.96 0.03 1.00 0.36 0.36 * 1.62 0.97 0.03 1.00 0.40 0.40 * 1.26 0.97 0.03 0.91 0.23 0.21 * 1.02 0.97 0.03 0.23 0.05 0.01 1.04 0.97 0.03 0.32 0.06 0.02 1.19 0.98 0.03 0.80 0.18 0.14 1.11 0.98 0.03 0.55 0.11 0.06 0.89 0.98 0.03 0.42 -0.10 -0.04 TV 1.30 0.99 0.03 0.94 0.24 0.22 * 1.13 0.99 0.03 0.59 0.12 0.07 1.19 0.99 0.03 0.77 0.17 0.13 TV 1.28 1.00 0.03 0.91 0.22 0.20 * 1.04 1.00 0.03 0.20 0.04 0.01 1.53 1.00 0.03 1.00 0.34 0.34 * 1.26 1.01 0.03 0.87 0.20 0.17 1.11 1.01 0.03 0.44 0.09 0.04 1.34 1.01 0.03 0.96 0.24 0.23 * 1.26 1.02 0.03 0.86 0.19 0.16 1.04 1.02 0.03 0.11 0.02 0.00 1.21 1.02 0.03 0.76 0.16 0.12 1.17 1.03 0.03 0.63 0.12 0.08 0.81 1.03 0.03 0.83 -0.27 -0.23 1.09 1.03 0.03 0.25 0.05 0.01 1.23 1.04 0.03 0.78 0.16 0.13 1.36 1.04 0.03 0.96 0.24 0.23 * 0.87 1.04 0.03 0.71 -0.20 -0.14 1.09 1.05 0.03 0.19 0.04 0.01 1.00 1.05 0.03 0.24 -0.05 -0.01 1.36 1.05 0.03 0.95 0.23 0.22 * 1.30 1.05 0.03 0.87 0.19 0.16 1.15 1.09 0.02 0.32 0.06 0.02 1.26 1.09 0.02 0.72 0.13 0.10 1.40 1.09 0.02 0.96 0.22 0.21 * TV 2.57 1.09 0.02 1.00 0.58 0.58 * 2.77 1.10 0.02 1.00 0.60 0.60 * TV 1.51 1.10 0.02 0.99 0.27 0.27 * 1.30 1.10 0.02 0.80 0.15 0.12 1.34 1.10 0.02 0.87 0.18 0.16 1.30 1.10 0.02 0.79 0.15 0.12 1.06 1.11 0.02 0.21 -0.04 -0.01 1.30 1.11 0.02 0.78 0.15 0.11 1.19 1.11 0.02 0.40 0.07 0.03 1.23 1.11 0.02 0.56 0.10 0.06 1.19 1.11 0.02 0.38 0.06 0.02 1.40 1.12 0.02 0.94 0.20 0.19 * 1.00 1.12 0.02 0.55 -0.12 -0.07 TV 1.32 1.12 0.02 0.80 0.15 0.12 TV 1.87 1.12 0.02 1.00 0.40 0.40 * 1.62 1.12 0.02 1.00 0.31 0.30 * 1.36 1.13 0.02 0.87 0.17 0.15 1.06 1.13 0.02 0.32 -0.06 -0.02 0.96 1.13 0.02 0.73 -0.18 -0.13 1.72 1.13 0.02 1.00 0.34 0.34 * 1.06 1.13 0.02 0.34 -0.06 -0.02 1.28 1.13 0.02 0.64 0.11 0.07 1.04 1.13 0.02 0.45 -0.09 -0.04 1.28 1.14 0.02 0.63 0.11 0.07 1.34 1.14 0.02 0.81 0.15 0.12 1.26 1.14 0.02 0.55 0.09 0.05 1.21 1.14 0.02 0.36 0.06 0.02 1.26 1.14 0.02 0.54 0.09 0.05 1.40 1.14 0.02 0.91 0.19 0.17 * 1.00 1.14 0.02 0.64 -0.14 -0.09 0.98 1.14 0.02 0.71 -0.17 -0.12 Table 1: A piece of output of the method","tags":"machine-learning","url":"tv-ad-attribution-gaussian-processes.html"},{"title":"On the ethics of CRISPR","text":"I was reading this article on the ethics of editing human genome and I realized there's a missing point in there. CRISPR in short is a technology that allows us to edit our own genome. Of course it has countless number of useful applications as it's very simply depicted in the above picture (credit: economist). But recently Chinese scientists have genetically modified human embryos ( link ). Fortunately there's been some discussion going on in the community about the ethics of editing human embryo's genome, or in general human genome. The question I want people to think about is: who should/will have access to this technology? Obviously this technology will be very expensive when it reaches the market. Does it mean only rich people will be able to use it? And I'm not talking about the applications which concern diseases; I'm rather talking about editing your child's genome, to make it smarter, or stronger, or whatever you feel like it. We already have too much of a difference between classes in our society. This difference has been empowered partially, if not mostly, by the fact that people have been able to accumulate wealth through time and even generations. If we allow them to be the ones benefiting a technology like CRISPR , we will practically give them the power to annihilate the lower classes through time. One piece of good news is that some people are talking about how to regulate this sort of technology ( link ). I believe we should start a conversation about who should have access to the technology. This concern is regardless of how and what we decide about the ethics of the method for different applications. If we, as human species, decide not to use it at all, then nobody has access to it. But if we give it a go, then we should make sure all classes of the society have equal access to it. The least and the last we need is to have a rich sub-population with higher physical and mental capabilities. EDIT : Thanks to Ian Sample, found a podcast by the guardian here .","tags":"ethics","url":"on-the-ethics-of-crispr.html"},{"title":"Zimbra Auto Provisioning from FreeIPA","text":"After quite a few days struggling to configure a Zimbra server so that it automatically fetches users from our freeIPA ( LDAP ) server, I finally managed to have a configuration which works. I got help from a bunch of pages like this and this one. This comes after you fix the external LDAP authentication and probably also external GAL configuration on your Zimbra server. zmprov gives you a nice terminal to configure the server: $ su - zimbra $ zmprov This is the set of commands I used to set it up: prov> md mysampledomain.net zimbraAutoProvAccountNameMap \"uid\" prov> md mysampledomain.net zimbraAutoProvAttrMap \"givenName=givenName\" prov> md mysampledomain.net +zimbraAutoProvAttrMap \"sn=sn\" prov> md mysampledomain.net zimbraAutoProvBatchSize 80 prov> md mysampledomain.net zimbraAutoProvLdapAdminBindDn \"uid=mail_server,cn=users,cn=accounts,dc=mysampledomain,dc=net\" prov> md mysampledomain.net zimbraAutoProvLdapAdminBindPassword \"myverysecretpassword\" prov> md mysampledomain.net zimbraAutoProvLdapBindDn \"uid=mail_server,cn=users,cn=accounts,dc=mysampledomain,dc=net\" prov> md mysampledomain.net zimbraAutoProvLdapSearchBase \"cn=accounts,dc=mysampledomain,dc=net\" prov> md mysampledomain.net zimbraAutoProvLdapSearchFilter \"(&(ObjectClass=person))\" prov> md mysampledomain.net zimbraAutoProvLdapStartTlsEnabled TRUE prov> md mysampledomain.net zimbraAutoProvLdapURL \"ldaps://ipa.mysampledomain.net:636\" prov> md mysampledomain.net zimbraAutoProvPollingInterval \"10m\" prov> md mysampledomain.net zimbraAutoProvScheduledDomains \"mysampledomain.net\" prov> md mysampledomain.net zimbraAutoProvMode \"EAGER\" To diagnose why the system wasn't working, I also had to figure out where the log files are, and how to produce more logs. Oddly enough, they're not in /var/log , and instead they are written by default in /opt/zimbra/log/mailbox.log , or other related files in that folder. I added log4j.logger.zimbra.autoprov=TRACE at the end of my /opt/zimbra/conf/log4j.properties file, which will be overwritten next time I restart the services using the configurations in /opt/zimbra/conf/log4j.properties.in . Finally to make the logging system reload the logging configuration, you need to run zmprov rlog . You find more info here .","tags":"sysadmin","url":"zimbra-auto-provisioning-from-freeipa.html"},{"title":"synapse.org","text":"I decided to actually write on this blog (decision was made this morning and is final :D ). On June 2nd I received an email from my adviser telling me about the DREAM Challenges . I kind of liked it as some sub-challenges fit exactly what I've been doing for that past few months, and that project is already in a phase that we've submitted a manuscript about it. So why not? Going through their pages, I realized they've got a python client to interact with their databases; that's cool, but it didn't support python 3. As a result I made this pull request and at the moment of writing this post, they seem to be interested in accepting the commits (although the changes are not perfect and/or complete and more work is required). Later I was reading challenge descriptions which made me interact with the website for a while, and that made me a bit frustrated. I have to say, I'm not a web-developer and I'm talking as a simple end user. If I was the web-site's development manager, I'd add these to the first possible sprint : Add a proper navigation system to the website; currently you can only easily navigate within a challenge/project. You need to go to the home page first to go anywhere else. Try to go to DREAM challenges, you need to go to the home page, wait for that timer based frame to come which has the link to the challenge, click on it. In my browser (chromium), I have to refresh the whole page if I miss it once, or wait for it to come again. So: add proper links, menus, or whatever you think suits the framework for a nice navigation to projects. Within a challenge description page, click on a link and the whole page will be reloaded, slowly and painfully. For some reason the layout of the page changes through time after each part of the page is loaded. A simple markdown based static website works much faster and better, if you want it to be fancy, add proper AJAX components/codes to make it smooth and fast. The website has serious performance issues and seems unnecessarily heavy. I'm pretty sure developers have their reason for having the website the way it is, but those reasons must be justified/changed.","tags":"Blog","url":"synapseorg.html"}]}