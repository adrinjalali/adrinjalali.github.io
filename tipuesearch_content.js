var tipuesearch = {"pages":[{"title":"Curriculum Vitae","text":"Source: https://github.com/adrinjalali/cv/blob/master/adrin-jalali.pdf function renderPDF(url, canvasContainer, options) { var options = options || { scale: 1.5 }; function renderPage(page) { var viewport = page.getViewport(options.scale); var canvas = document.createElement('canvas'); var ctx = canvas.getContext('2d'); var renderContext = { canvasContext: ctx, viewport: viewport }; canvas.height = viewport.height; canvas.width = viewport.width; canvasContainer.appendChild(canvas); page.render(renderContext); } function renderPages(pdfDoc) { for(var num = 1; num <= pdfDoc.numPages; num++) pdfDoc.getPage(num).then(renderPage); } PDFJS.disableWorker = true; PDFJS.getDocument(url).then(renderPages); } renderPDF('/files/adrin-jalali.pdf', document.getElementById('holder'));","tags":"misc","url":"pages/curriculum-vitae.html","loc":"pages/curriculum-vitae.html"},{"title":"Get in Touch","text":"E-Mails adrin.jalali &#94;as-usual&#94; gmail.com Profiles github Linkedin stackoverflow Blogs If you are looking for a work related blog of mine, you are surfing it right now. If you are curious to know what I would probably say outside work, you can check out Adrin - The Idiot","tags":"misc","url":"pages/get-in-touch.html","loc":"pages/get-in-touch.html"},{"title":"Home","text":"Who You are surfing Adrin Jalali ‘s page, who works as a data scientist/data science consultant and has spent his PhD (not finished yet) doing research in the field of Computational Biology. Work I got to data science from the data and AI . My first implementation of a single hidden layer Perceptron neural network was in 2001, and in 2002 I developed a constrained based optimization using genetic algorithms which I then used as my bachelor's thesis' project in 2006. I wrote and sold a database related software in 2002 when I was still in highschool and it was operational for at least 10 years after it was deployed. By the time I wanted to start my PhD in 2011, I had worked with future stock market data for an automatic trading software, and had worked on bank transaction data to detect/prevent fraud offline and in real time. These latter projects pushed me towards data science, and that was when I got into data mining . During my PhD I worked on disease/patient related data and I became more and more interested in healthcare related issues. Since mid 2016 I started working more or less as a consultant solving different problems for different projects, such as NLP and time series related projects. However, I can always go for a more fulfilling job which I'd find positive for humanity. I may start a project on my own or with some others soon, but regardless, since I get this question very often, here's a list of factors I find important in a job and the environment around it: It needs to be positive in my system of thought, and here are some examples: I get too many requests from the financial sector, but unless the project directly aims to reduce wealth gap and make the wealth distribution more flat (refer to Piketty's \"Capital in the 21st Century\") in one way or another, I'm not interested. An online retail store pushing for ethically produced goods with good labour conditions is a good thing, but not necessarily my passion. An organization working on the health situation of poor urban areas, or developing countries, developing products using local workforce all as a non-profit organization, is a place in which I could happily contribute to the cause. Then it's the tools/technology used in the team and for the product. I prefer to almost exclusively work using open source solutions, and my job almost certainly has to include some contributions into the open source community in one way or another. This means it's a no go for a Windows/MacOS workstation and/or proprietary software, and it's cool if it's a GNU /Linux workstation while using Python/C++/R. And the last but not least, it's the work environment. Sure it needs to be nice with all the usual perks, but more importantly I care about the people in the workplace and its gender gap. It takes a lot for me to be convinced that a team of 10 male engineers/scientists does not have sexist attitudes. I can keep most of my politics out of the workplace, but feminism is not one of them. Area of research I use/design machine learning tools to classify samples. The datasets I work on vary from microarray to DNA methylation data. I am mostly focused on using gene/protein networks in order to help the classification task, while keeping in mind I need to interpret my results for a biologist. Therefore my goal at the moment is to have a method which directly uses or is inspired by the biological networks, classifies my samples, and can be interpreted on the biological level. [ Poster ] Adrin Jalali, Nico Pfeifer, \" Analyzing How Protein Interaction Networks Improve Classification Performance in Gene Expression Data Analysis \", ISMB / ECCB 2013, Berlin, Germany. Jalali A., and Pfeifer N., \" Interpretable per Case Weighted Ensemble Method for Cancer Associations \", BMC Genomics, volume 17, no. 1, 2016. I also worked on flow-cytometry data which is a single cell level data. I did it mostly when I was for 15 months in Vancouver, Canada, and I was doing research in British Columbia Cancer Research Center and The University of British Columbia . As a result of this part of my research, I can refer you to: Kieran O'Neill†, Adrin Jalali†, Nima Aghaeepour†, Holger Hoos, and Ryan R. Brinkman. \" Enhanced flowType/RchyOptimyx: A Bioconductor pipeline for discovery in high-dimensional cytometry data. \" Bioinformatics (2014), doi: 10.1093/bioinformatics/btt770 Nima Aghaeepour†, Adrin Jalali†, Kieran O'Neill, Pratip K. Chattopadhyay, Mario Roederer, Holger H. Hoos, and Ryan R. Brinkman. \" RchyOptimyx: Cellular hierarchy optimization for flow cytometry. \" Cytometry Part A 81, no. 12 (2012): 1022-1030, doi: 10.1002/cyto.a.22209 Nima Aghaeepour, Pratip K. Chattopadhyay, Anuradha Ganesan, Kieran O'Neill, Habil Zare, Adrin Jalali, Holger H. Hoos, Mario Roederer, and Ryan R. Brinkman. \" Early immunologic correlates of HIV protection can be identified from computational analysis of complex multivariate T-cell flow cytometry assays. \" Bioinformatics 28, no. 7 (2012): 1009-1016, doi: 10.1093/bioinformatics/bts082 [ Poster ] Adrin Jalali, Nima Aghaeepour, Kieran O'Neill, Andrew P. Weng, Ryan R. Brinkman, \" Analysis and Classification of Lymphoma Sub-types \", British Columbia Cancer Agency Annual Conference, Victoria, BC , Canada, 2012. †: equal contribution List of publications available on Google Scholar .","tags":"misc","url":".","loc":"."},{"title":"scikit-learn sprint at Nairobi, Kenya","text":"Almost a year ago, after being the co-speaker of a \"My first open source contribution\" talk at PyData Berlin 2018, I myself became very motivated and started actively contributing to the scikit-learn project. I was surprised to see how much I could and had to learn to improve my contributions, and that was after over 20 years of programming experience, 6 years of which I did mostly Python, and several years of working in the industry. It wasn't even the first time I was contributing to an open source project, but it was the first time I was actively looking for issues to fix. One of the reasons I stayed on the project, was the extremely nice and patient attitude of the reviewers and core developers of the project, most importantly, Joel Nothman. I felt welcomed, tutored, and guided throughout my contributions. Of course it also required my patience, since some of the contributions would go through a long iteration process, even though they were only a few lines of example code. After a while I was more confident taking over some of the more challenging tasks, and it felt more rewarding as they became more challenging. One day in December (2018), I woke up to find an email in my inbox which said they'd like to have me as a core developer, and if I'd accept. There are very few instances in my life in which I remember such a feeling of happiness flowing through my whole existence. I accepted with joy, and there I was with a whole bunch of new responsibilities added to my shoulders; but the kind which make me want to wake up in the morning to take care of. It also happened to be the case that the core team was planning a sprint in Paris to go be together and work on issues in person for a few days. I joined that sprint, in February, where we worked for a whole week, 9am to 10pm more or less. It was so stimulating that I did not realize how exhausted I had become. When I went back home, it took me quite a few days to recover, but it was one of the best weeks of my life, working with brilliant people from all around the world. It was also really good for the project, since we managed to solve some of the very long standing issues of the project, which had been open for years. It was a while after that when I saw an email in the mailing list, asking if somebody could give a hand for a sprint being planned in Nairobi, Kenya. It was going to be organized by Women in Machine Learning and Data Science ( WiMLDS ), and since it was the combination of two things very close to my heart (diversity and open source), I made sure to respond to it and offer my help ASAP . Reshama Shaikh, the main organizer from the WiMLDS side, got back to me shortly and we started planning for the sprint. What I did NOT know, was that they were planning to fly a contributor to Nairobi to be there in person for the sprint, which I found a very nice surprise. I think it took around 4 months of planning which I was involved with, including vaccination, visa, etc. I don't think we even planned the sprint in Paris this much, but Reshama and Mariam Haji (the main organizer in Nairobi) were both very thorough and planned everything to the last bit of it. So I flew to Nairobi, stayed there for 2 days, and had the sprint on a Saturday. We ended up having about 40 attendees, almost all new to open source, and by the end of the day we had over 20 open pull requests (PRs) generated by them on that single day. It's so gratifying to see the joy in people's face when they open their first PR and receive feedback from the core developers. I always love to see their faces when they start touching the files of the package, which they've used till then as a user. I still remember the feeling of sending a patch for an open source project for the first time (before git was invented), and when I see those same feelings in new contributors faces, it pushes all the exhaustion out of my being. I'd say it was a very successful sprint, specially since quite a few of the contributors we had there, are still contributing and opening new PRs even afterwards. To me, that's what a sprint for new contributors is for, to enable them and break all the barriers they feel having in front of them, and have them motivated enough that they continue contributing afterwards. However, I think there are still a few things which could be improved and I'm noting them here mostly for my own future reference. There is a trade off between how much information the instructor tries to give to the audience, before they jump into finding issues to work on, and how much time is left for them to contribute. I kept the introduction very short, and just pointed them to the part of the documentation where they could follow the instructions and start working on issues. I did that mostly because I personally don't find those slide based talks very productive. However, I figured most people were lost, and they were not even sure which question to ask or where to start. What helped, was that I found a minor issue in the documentation, and fixed it and submitted a corresponding PR , while sharing my screen. Immediately after that it felt like the room now knows much better what they're doing. I would have one of those ready next time, and have kind of a hands on tutorial on contributing, git , and GitHub instead. Another thing which I think we should improve in sklearn documentation, is the guide for people to setup their environments. There were people who were still trying to setup their environment and get the package to compile still at the end of the day, and that to me is a strong hint that something could be improved. Another point is that it was a weekend, and most core developers were either sleeping or not at their PCs. Being the sole person trying to review the PRs as they come and at the same time to answer the questions on-site, is almost not manageable. I'd try to coordinate with at least one other core developer to be online during the sprint, before the sprint this time. Overall, I loved everything related to this sprint, and I hope we get to do it again next year :)","tags":"open-source","url":"scikit-learn-sprint-at-nairobi-kenya.html","loc":"scikit-learn-sprint-at-nairobi-kenya.html"},{"title":"Ways to contribute to open source projects!","text":"Writing code is not the only way you can contribute to an open source project. Like any other project, there are many tasks which are not programming, but are required to push the project forward, and many programmers are not even necessarily good at those tasks. Think of organizing meetups, UI / UX design, documentation, and translation as some examples. You can read more about these here . This post is by far not an exhaustive list of what can be done, rather what I have personally been engaged with. Let's assume you start using a piece of software as a library in of your projects, and through time you want to get engaged with it more. Please always remember that many of these packages are maintained by volunteers and they're not always immediately available to respond to your questions, suggestions, or contributions, so be patient. As a user, one of the first things you may do is to read the documentation of the package (and hopefully that exists for the package you're going to use). For most libraries out there, the documentation may be sparse, incomplete, or at times incomprehensible. Although in some cases it may be the case that you need to cover some background to better understand what you're reading, but in most cases it's the documentation you're reading which is not perfect. Unfortunately, it's not easy to convince core developers to write good and extensive documentation, since they're mostly busy implementing new features or fixing reported bugs and issues. Some packages allow you to suggest an edit to the documentation on the fly, from the browser itself, which removes the initial barrier for many to start contributing. Elasticsearch is a good example. As a new user, you probably pay attention to the documentation probably more than anybody else. Therefore you may find mistakes or see potential easy improvements which are usually not noticed by other more experienced users and developers. Be encouraged to suggest any change you deem appropriate. It may be accepted immediately, or it may start a whole conversation about a certain aspect of the software. Some of my contributions have been as small as a punctuation, to make the sentence a bit easier to understand.","tags":"open-source","url":"ways-to-contribute-to-open-source-projects.html","loc":"ways-to-contribute-to-open-source-projects.html"},{"title":"How to find a good open source project for contributions?","text":"When looking for a project to which you'd like to contribute, there are two major aspects you may want to consider. One is the way it's released and managed, and the other is the community around it. The way I see the first aspect, it's like a spectrum. Closed source proprietary software on one side (let say the right side), and community driven and fully transparent and open sourced project on the other side (let say the left side). And it's important to realize that different projects are somewhere on this spectrum, and not necessarily on either of the two ends; let aside the whole licensing issue which itself complicates matters by another order of magnitude. Sometimes companies \"release\" the source code of a product, or a part of a product. Microsoft does this pretty often. You can only look at the code; you can't run it, you can't use it, and most people won't. To me it's more like a public relations thingy. They use it as a tool to gain reputation among the community. I see such examples as the ones pretty close the right side of the spectrum. Going slightly to the left side of the spectrum, there are projects which are \"released\" as a whole, and they may even include a friendly license letting you use it under certain conditions. They may even have a GitHub account and dump the source code there. Some of these projects are released once, and never maintained further, and some go through periodical releases; but they still keep the public community mostly in dark about the development of the new features. They usually also don't really welcome contributions from \"outsiders\", and that's not because they don't like contributions; it's mostly due to the fact that they have limited amount of resources and their priorities are dictated by the products in which these pieces of software are used. I'm not a big fan of contributing to these projects since a large part of the conversation about the project happens within the team inside the company, and external contributors are treated somewhat like a second order citizens to these projects. I get the feeling that projects such as spaCy or tensorflow are such projects. They have an extremely slow response rate on community questions. You do get the feeling that it's a one sided relationship, i.e. they release a version, and if you're lucky, your concerns may have been addressed in the release. The next group of projects are the ones which are considered as a part of free and open source software ( FOSS ) . This means not only the software is free and open source, but also volunteers are encouraged to contribute to it and help it be improved. Naturally, if you'd like to work on a project as a volunteer, these are good candidates. But even within this category of projects, not all of them are a friendly place to contributors. Fortunately, more and more projects are moving towards having a proper code of conduct ensuring a harassment free environment, and probably the most high profile one among the recent ones is the Linux Kernel project, which recently confronted Linus Trovalds about it and adopted a new code of conduct . It is important to realize that there's no such thing as the open source community. The reality is that there are different communities around different projects/topics. People's attitudes differ from community to community, programming language to programming language, etc. My personal experience, which admittedly doesn't necessarily project the complete picture about any community since it's by definition subjective, has been the following: C++ community has always been the harshest in my experience (and I know I'm not alone having experienced that). If you ask a question there which sounds stupid to the experts in the field, they probably would crush you by asking why you haven't read a \"proper\" book on C++. By that they usually mean the whole C++ ISO standard. R community was really unpleasant to interact, to the point it encouraged me to switch from R to Python (yeah, that is in fact why I became a python coder). After creating and maintaining three packages for R in CRAN and Bioconductor, I was kinda done with it. I need to mention that this was back in 2012, and a lot could have changed in these few years. I have no idea how the community behaves these days; and I'd be delighted if I'm completely wrong about the community now. Python has always been nice as far as I'm concerned. It is a language which has it's own code of conduct . You can imagine how that has influenced the whole community around it. Here are a few things you can consider when checking on a [ FOSS ] project: Activity : Is the project under active development? When was the last release? How often people open issues and contribute to the project? How often do main developers respond to those issues? Many projects may seem pretty cool at the first glance, but when you dig deeper, they may be dead projects. Sure you can try to revive it, but that may require more dedication and time than what you have at hand. Code of Conduct : The presence of a code of conduct is neither a necessary nor a sufficient condition for a healthy atmosphere in a project, but it's a sign that the core developers do care about the kind of atmosphere around the project. Also, the more people care about codes of conducts, the more pressure on the projects to adopt one. Community Contributions : For larger projects, you can check how open the community is, by checking the repository logs. There are many tools to visualize activity on repositories, for instance, you can check the activity around tensorflow here , and activity around scikit-learn here . You can easily notice how tensorflow has a special user called tensorflow-gardener , and you can read about it here . I personally prefer repositories which have a larger set of contributors publicly contributing to the ones hiding internal development behind a special user. Treating Contributors : It's never pleasant to work with a bunch of snobs! As a new contributor, you are bound to make many mistakes, or try things the way which is not the common way it's done in the project you're working on. This is normal and expected as users join a community. It is very important for the core developers of that project to know that and treat newcommers nicely and encouragingly. You can see if that's the case or not, by going through some of the issues and pull requests in the project you're checking. Just take your time and read some of the conversations happening around some of the issues to make sure it suits your taste. You can also check the \"closed\" issues to see how the core developers handle closing issues. In another post I'll talk about ways you can start contributing to a project of your choice.","tags":"open-source","url":"how-to-find-a-good-open-source-project-for-contributions.html","loc":"how-to-find-a-good-open-source-project-for-contributions.html"},{"title":"Why would you want to contribute to an open source project?","text":"I've been a fan of open source software for a long time. However, up until recently, I wasn't seriously contributing to any specific project; but why would you want to contribute to an open source project in the first place? There are a few different aspects to be taken into account here, and here I try to go through some of them. Help the community by contributing to the project. This is the one which comes to one's mind the most. You like a product, or a community around a particular software, hence you contribute to that particular software. Doing so, you help yourself (if you use the software) and everybody else who uses that same product. Help the larger community by advocating and normalizing open source contributions. When you contribute to open source projects, you become yet another person who does that and it demystifies open source contributions for people around you. Many people (if not most) think you need to be a proficient programmer for an open source contributions, not realizing many contributions are not even in the form of code. If you work in a place where you use those projects in your products, it would make you and your colleagues realize how much work goes into those projects, and how your company could benefit from spending time improving it as a part of your job. Personal recognition due to those contributions. Although your first contributions may be small, they'll grow and become more substantial if you stay persistent in your contributions. Sooner or later you become a core member of the community you work with, and you'll be recognized as such. Unlike your contributions to the closed source products in whatever company you work, the open source contributions are out there and you don't ever need to prove that you've done them. It becomes a provable part of your professional portfolio. Free mentors while your contribution gets reviewed. This is only true for the friendly and nice project communities, and fortunately their number is growing. For quite a few years, if not decades, open source communities were mostly toxic, but that's changing and people are becoming nicer and more patient with newcomers. This means you shouldn't fear contributing, since the core developers of the project would help you go through the process and get your contribution up to the standards of the project. This also helps you gain experience working in a larger and more diverse team, and feel all the perks and challenges that come with it. Unfortunately not many places encourage their employees and colleagues to have open source contributions, but still, I don't know of a place which would dislike a candidate who has had some contributions which they can see and check. Maybe half the job offers I get are because of my presence in the open source world. Personally, I don't accept a job offer which involves no open source contributions, and I always check what contributions the company and their employees have. At least in Berlin, the scene is changing fast, and it's becoming somewhat embarrassing for companies not to have any open source product, and many of them are working on improving on this front. This is partly due to the fact that many developers prefer an open and transparent company, and open source is just a part of it. So at the end, the more people contribute to these projects, the more pressure it puts on companies to include themselves in this movement, which benefits the larger open source software community. I'll talk about what these contributions mean, and how one could start contributing in a separate post.","tags":"open-source","url":"why-would-you-want-to-contribute-to-an-open-source-project.html","loc":"why-would-you-want-to-contribute-to-an-open-source-project.html"},{"title":"VectorFight - Winning \"Hacking Global Health\"","text":"Background A weekend in October (15-17.10.2017), on the side of the 9th World Health Summit ( WHS ) I attended a hackathon called Hacking Global Health . We were nearly 40 people, in about 20 teams, and each team had to pitch an idea in 2 minutes. The only constraint on the ideas was that it had to do with improving health in poor urban areas. The trick was that only 8 of these teams were supposed to go to the \"next\" round and develop those ideas for the final pitch. But that was supposed to happen organically, without any interference from the judges or organizers. This meant that we had around 15 minutes or so, to form 8 teams on our own. The organizers, however, tried to push people with different backgrounds into the same teams, such that each team has all the required skills. This is how I ended up in a team which needed somebody with \" IT \" background. They were people from two different teams, one with a focus on malaria mosquito traps, the other focusing on plague carrying rats. So I abandoned my idea, and we joined forces to come up with something. A bit later two more joined us with a business background. After merging two of the ideas, we came up with a platform to educate people and improve their situation regarding vector borne diseases. It was a fantastic experience for me, since I didn't even know this meaning of the word \"vector\" beforehand. Our team was fantastically open about the ideas, and that's why we ended up with an idea which was none of the original ones, but one covering two of the original ideas. The final pitch was so different from any of the original pitches, that we had to resubmit a pitch form to put in the list of final pitches. At the end, according to the judges, it was a clear and easy decision to choose our team as the winning team, for which we're really proud. Idea The idea is a platform which gives information about vectors and their related diseases to the users, and it receives information about those vectors from the users and other sources such as government organizations or research institutes. It has two different faces: A smartphone app which enables the community to report potential breeding sites such as potholes and communicate with each other in order to potentially take action and fix them. They also receive information about vectors and diseases most relevant to them. The app serves both as a mean to disseminate information, and to collect data. It also would try to build a community around these issues. A web interface/ API enabling different organizations to interact with the platform and use the data uploaded by the users. They would get a clear overview of how a specific vector/disease is spreading through time. Here's an overview of the ecosystem around the idea: And the interface of the solution would resemble: And here ‘s the presentation, the result of two hard working days. After the hackathon, we presented the idea at the conference itself, and a few days later at a German African health collaboration conference. A few months later we were invited to present the idea during a German Health Partnership meeting having quite a few key players of healthcare in Germany present. Unfortunately we never got a proper backing from the public sector, and the idea more or less died. I hope someone revives it some day!","tags":"healthcare","url":"vectorfight-winning-hacking-global-health.html","loc":"vectorfight-winning-hacking-global-health.html"},{"title":"A Criticism of \"Detecting Sexual Orientation Using Neural Networks\"","text":"I'd like to talk about this study: Deep neural networks are more accurate than humans at detecting sexual orientation from facial images , and it's not going to be a praise of the research! Damaging Message I put this study in the same category as studies trying to argue women are different than men, usually in derogatory ways, e.g. proving they're weaker, less intelligent, or worse at math. Recently we had the google engineer citing a whole bunch of them to support his argument about why men are better coders, or men are better in tech, or whatever bullshit he wanted to argue for. Honestly I won't bother reading it; I mostly heard about it in podcasts and read about it in the news. My point is, those gender related studies tend to be more damaging than beneficial, and I see no point in doing them. I understand this is a very consequentialist way of assessing the value of the work, but I'm comfortable with it for now. It's the same with the study in question. What's the point of having a module predicting people's sexual orientation given a face image, other than some other people using it in an argument such as: \"Your face shouts you're gay, scientifically proven\"!!! I'm sure our dear homophobes out there can think of more damaging ways of using the results of this research. This is all I could come up with! Methodology I don't even know where to start. I'll split these points into the ones questioning the validity of the study, and the ones which are simply poor wording. Questioning Validity AUC calculation In a classification problem, you'd use your module to predict the outcome, draw the ROC , and calculate the Area Under ROC ( AUC ). Pretty straightforward for a binary classification, which is the case in this paper. What I don't understand, is this randomly choosing a straight person's photo to compare against a homosexual, to calculate an AUC . I simply don't understand why you'd do that, and as a result, I can't assess the performance measures reported in the article. Variance of the accuracy They report one AUC performance for those 20 folds. I guess the results are accumulated over 20 folds and one single AUC is calculated overall. This is important, because the calculated performance usually highly depends on the random split of the data. Therefore without a data showing the distribution of the calculated AUCs, one single number is not representative of how reliable the model in reality is. SVD on each fold separately or not? In any data analysis, it is crucial to apply [virtually all] preprocessing steps only on the training data. Otherwise you're already seeing the test data before even starting you training, and that's cheating. The text is not clear weather they run a separate SVD for each of those 20 folds they have or not. If they do SVD once and use the same features on all those folds, the results are immediately invalid. I cannot tell from the text what they've done, and I couldn't see a link to their source code. My guess is that they are actually doing it the right way, but it's not written clearly. Model Interpretation Quoting the article: The gender atypicality of gay faces extended beyond morphology. Gay men had less facial hair, suggesting differences in androgenic hair growth, grooming style, or both. They also had lighter skin, suggesting potential differences in grooming, sun exposure, and/or testosterone levels. Lesbians tended to use less eye makeup, had darker hair, and wore less revealing clothes (note the higher neckline), indicating less gender-typical grooming and style. Furthermore, although women tend to smile more in general (Halberstadt, Hayes, & Pike, 1988), lesbians smiled less than their heterosexual counterparts. Additionally, consistent with the association between baseball caps and masculinity in American culture (Skerski, 2011), heterosexual men and lesbians tended to wear baseball caps (see the shadow on their foreheads; this was also confirmed by a manual inspection of individual images). The gender atypicality of the faces of gay men and lesbians is further explored in Study 2. Comparing these factors with the reported AUC , you could argue that social trends and stereotypes followed by many people, has a substantial factor in giving the classifier its power. To better understand my point, assume you want to train a classifier predicting whether or not a person in a picture is a transgender. Since many transgender people follow an exaggerated version of the opposite gender's stereotypes in the society, and the skull itself is predictive of the sex of the person to a good extent on its own, the model would probably have an easy time giving the right outcome. What I mean, is that if you have a model which doesn't take into account the confound social variables to predict a certain variable, your results are crap! Poor wording Not a Deep Neural Network ( DNN ) Method The article suggests DNNs can achieve the reported performance, while using a DNN only to extract features, and then feeding them to an SVD -> LASSO ->logistic regression pipeline. How is that a DNN method? I can only imagine they've chosen to advertise it as such, due to the hype around deep learning; after all, who would dare to argue with a DNN these days? Basically, the statement \"deep neural networks can detect sexual orientation from faces\" is a complete misrepresentation of the research. LASSO and alpha They say they use LASSO with alpha=1 for feature selection. The alpha they refer to is a parameter of elastic net, not LASSO , which results in LASSO when set to 1. In other words, LASSO ALWAYS has that alpha equal to 1, otherwise it's not a LASSO . Final Remarks I guess at the writing of this post, the article has not been accepted and it may be subject to many changes; in which case, I hope reviewers find the problems I found and many more, so that the final version of the article is a much better work than what I read. But even then, I have a hard time understanding the reason behind such studies. I'm sure there are much better and more beneficial ways of analyzing the same data and coming up with much better conclusions. Also, I could only analyze this research in depth only if I had the code and the data publicly available. One may argue that I can contact the authors asking for the code and the data; but that's not what I call \"publicly available\"!","tags":"ethics","url":"a-criticism-of-detecting-sexual-orientation-using-neural-networks.html","loc":"a-criticism-of-detecting-sexual-orientation-using-neural-networks.html"},{"title":"A Central Cancer Diagnostics Hub","text":"Although the context of this post is bioinformatics and cancer, it applies to many other fields as well. I've had this idea for a while, and this an effort to make it more concrete. In this post, a method refers to a computational model or an algorithm, from the preprocessing phase to the final result. Motivation The idea is motivated by my experience in bioinformatics, dealing with cancer data and cancer related questions such as cancer diagnostics, while being in a cancer hospital observing some of the struggles oncologists and pathologists face. I'd like to address the following challenges: Reproducibility crisis in the field, which I talked about in more detail here Reinventing the wheel (over and over again). In science to show the merits of your work, you most probably need to compare it with other methods. Since most methods are not open source, and even when they are, it's not a trivial task to use them on your data, you end up at least partially re-implementing those methods yourself. The gap between research and clinics. Again, since it's not easy to use most published methods on your data, it's not easy for clinics to take a publication and use that method on patients' data. Players Ideally there would be a place facilitating method development as well as their usage. To understand the proposed system, consider the problem of predicting the subclass of different cancer types, and the following three players or stakeholders. The programmer, or the bioinformatician The system provides certain APIs for programmers to deliver tasks related to cancer diagnostics. These tasks are steps along the way of the analysis. Preprocessing of the data is one step, so is predicting the sub-class of a cancer for instance. Programmers don't have to implement the whole pipeline; they can focus on a method, for example, which relies on a certain preprocessing model which itself has already been implemented on the existing datasets. Or in contrast, a researcher can implement a preprocessing technique and test it in combination with different methods available in the repository of the system. The oncologist, pathologist, or a clinic Once a method is uploaded to the system, it can be trained on different cancer types, and then a clinic or a hospital can use the system to get suggestions on the subclass of the cancer of a newly acquired sample for a patient. They can also check if different methods agree with each other on their prediction. The system in this case plays the role of yet another expert in the counsel diagnosing a patient, or to help an oncologist if the patient's disease is not an easy case to diagnose. In many hospitals oncologists already use the algorithms developed by the bioinformaticians working in their labs, but that's the limit they can reach and they don't have access to other labs' methods. Some other clinics/cancer centers have dedicated groups re-developing published works of other labs/researchers. This diagnosis hub would be a place for them to join their efforts. The lab/person producing data When a lab produces a new dataset, they usually need to analyze the data and explain how their dataset can be used and where its values are. After cleaning the data, this process involves preprocessing the data and then applying some machine learning techniques to it, to show and support a hypothesis such as \"measurement X is predictive of condition Y\". Without a place for them to upload and apply existing methods on their data, they need a computational biologist and a fair amount of time for the analysis to happen. Instead, they could upload the dataset to this hub and have different methods run against it, and then use the methods' outcomes for their analysis. Related and existing work There are many open datasets out there, but they are not necessarily nicely formatted and they don't follow the same guidelines to store the data. After a while, at least two efforts started to tackle this problem, one is ICGC in Europe, and the other one is TCGA in the US . These efforts store the data in a nicely formatted way, which makes it convenient to develop a method on one cancer type, and try it on the other ones. There are also places which have their own method run against different datasets, and you can browse their results and visualizations, or efforts and products enabling you to create your own platform and data storage. Some examples using the TCGA data are listed here . There are also products such as DNAnexus which provides related services, which helps researchers move their research to the cloud and have reproducible results, but it doesn't work as a hub serving all the abovementioned players. Conclusion We, as a society, need an open product and a team delivering such a service, which is easy to setup and use. I recently left academia, while still wrapping up my PhD thesis, and joined industry. I see very well where academia could use recent advancements in industry when it comes to big data and machine learning infrastructure, and also how clinics and the industry could benefit from a fast adaptation of recent academic advances. Note : I'll try to update this post as it develops.","tags":"science","url":"a-central-cancer-diagnostics-hub.html","loc":"a-central-cancer-diagnostics-hub.html"},{"title":"An Essay on the Reproducibility Crisis","text":"Reproducibility (in science) It's almost not a word, to the extent that as far as I know it's only used in the context of science, and it has its own Wikipedia page . In simple terms, a scientific research's results are reproducible if you can take its report, follow the instructions, and get the same or similar results. Reproducibility Crisis Most research cannot be reproduced, see this video , or articles like this one , this other one , or this . It wouldn't be a crisis if it weren't a systematic and widespread phenomenon. It is understandable if every now and then a researcher comes into a conclusion, or finds some results, attributing the results to what they think is the cause of what they've observed, and be wrong. But these kinds of mistakes are not why we see an abondant irreproducible set of publications. Why? How? Let's start with a simple example. Let say we want to study the influence of substance X (maybe a potential drug) on mice having disease Y. We take a group of mice, we make sure they have the disease, and then we give that substance to half of them, and see what happens. Now imagine we repeat the same study 10 times, and in 3 out of 10 times we repeat the same procedure, mice show some improvement in their condition when they are given substance X, and the other 7 times, we see no improvement, or even worse, we observe they die sooner compared to the mice given nothing other than their normal food. Now, having all the information, you might say okay, it seams the substance isn't really working. But if we only take the 3 replications of the same study in which we saw an improvement in the mice, then your information would be: \"we tried substance X for disease Y in several mice, and repeated the study 3 times, and in all those studies substance X helped mice dealing with disease Y\". Now you see a study which even has replicated the experiment 3 times, and in all of them substance X seems to be working. If someone in another lab sees this article, would think it works, and may try to replicate it. But we know there's 70% chance they won't see any improvement using X. This is an example of a study which cannot be reproduced with the claimed results. For the next example we need a bit of background. We, humans, have over 20,000 genes in our DNA , and we have the technology to take a sample from a part of our body, i.e. to biopsy, and measure the activity levels of all those individual genes in the given sample, hence having over 20K measurements for each given biopsy, usually referred to as gene expression data. Genetic abnormalities are observed in cancer. Cancer types and subtypes vary in those abnormalities, i.e. there is difference in different cancer types in terms of their genetic background. In order to study the genetic background of cancer, biopsies are taken from cancer patients, and their gene expressions are measured. But the data coming from this process is not perfect: Inter-cellular processes are stochastic, meaning they don't follow a deterministic pattern. You might take two biopsies from a tissue of a healthy person, and for many reasons observe different expression levels between the two. The process measuring the gene expression levels for many reasons adds noise to the data, which means if you repeat the process of measuring the expression levels twice from the same given biopsy, you might observe different values. The measurement process is also prune to batch effects, which means you might observe consistently different values comparing measurements taken in two different labs across the town. All the above reasons, plus the fact that most datasets include only a few hundred patients, i.e. rows in the data matrix, whereas we get +20K features, i.e. columns in the data matrix, for each patient, make it a hard problem to solve, and also hard to reproduce. Now assume you want to take a published computational model, which is basically an algorithm, and apply that to some dataset you have at hand. In many cases that publication does not include enough details about how to preprocess your data. This is the part that tells you how and if you need to transform your input, keep some and ignore others, and maybe combine some of those input features together in some way. Then there is the main part, which if you're lucky, you can implement. This means if there is enough detailed explanation in the publication, you can reproduce the results given the data used in the publication, which itself is not trivial, since many publications don't publish the whole data used in their analysis. There have been efforts by the community and journals to encourage and maybe enforce reproducibility ( example ). For instance, some journals require you to have the code available upon request. But in practice, that code is mostly only available while the paper is under the peer review process and it's lost afterwards. Even then, it's not always easy to re-run that code anyway; they might be tuned to the hardware infrastructure available to the researcher, and in some cases even links to some data files which were available on the researcher's computer and not released. Now let say, the publication uses only open datasets which you can easily download. And the code is available on some open repository, in such a way that you can reproduce the results of the publication on those datasets. For the reasons explained above, such as noise and batch effects, you might try the same algorithm on a different dataset of the same kind, i.e. same measurement tools and same cancer types, but achieve very different results than the one reported in the publication. In some other cases, the publication includes some claims and analysis across different cancer types, and applies the method on let say, 3 datasets of different cancer types. You might try the same method on 3 other datasets of other cancer types, and observe results which are nowhere close to the ones reported and claimed. Unfortunately the complexity and the nature of the problem is not the only factor in having irreproducible publications. Very often it is the case that journals and peer reviewers don't actually try to reproduce the results, since it takes time and the community won't value the time a researcher would spend on those tasks. In many other cases researchers try their methods on many datasets, and choose the ones that works best for them to report. These are publications that are in best case reproducible on the datasets they mention, but as a scientific method, they are not necessarily reproducible in an independent lab, or using other datasets. Negative result : Imagine you have an idea, like an algorithm, which you think might predict how well a certain drug will perform. If you try the method and it does not deliver the performance you thought it would, or it performs worse that existing methods and algorithms, then you have a negative result at hand. Some factors that contribute to this phenomenon are (add a sarcastic tone to most of what you read in this section): Publishing negative results gets you nowhere in the scientific community. You won't get grants publishing them, even if you can do so. Most journals don't accept your article if it's mostly about negative results anyway. Reproducing other people's work is not valued. You won't be a respected researcher if you spend a lot of time trying to reproduce other people's work. You are supposed to be independent and innovative, constantly producing new results and methods. A method working half the time is not impressive enough. That's why if your method is working on 4 datasets out of 9, you might just not report the 5 it didn't work on. Or if you want to look honest, you might only report one or two, and claim your method works most of the time. Interpreting the results in a positive way. In many cases, you might read the publication and think it's very impressive, but if you only look at the charts, numbers, and performance measures, you wouldn't think it's particularly a better method than others. Politics in science and peer review process discourages criticizing other people's work. Once you're at the bleeding edge of science, not many people in the world are working on what you work on. This means when you submit a paper for a peer reviewed journal, chances are your article gets reviewed by those people who you know. Although they won't see the names of the authors of the article, but they can easily guess. That's why when you encounter a paper which you cannot reproduce, you don't want to openly criticize it. After all, who needs more enemies. Yeah it's really sad and gloomy, but people are working on it, and we need much more to be done.","tags":"science","url":"an-essay-on-the-reproducibility-crisis.html","loc":"an-essay-on-the-reproducibility-crisis.html"},{"title":"Clue-WATTx Hackathon","text":"Hackathon To me, a hackathon is when a group of people gather for a day or two, working for a somewhat common goal, develop, have fun, meet new people, and to home. Unfortunately for whatever reason, most hackathons have become a competition, with a set prize, organized by some company. The good part is that some attendees might get some money out of the hackathon, but the down side is that it usually becomes a competition and people stop collaborating. I was very happy to see that was not very the case in this hackathon, and we had mostly a really nice atmosphere for the whole weekend. It was a pleasurable experience working and chatting with Clue and WATTx people as well as the attendees. Clue Clue is for people who experience menstruation cycles, or periods, to record their cycle, and related symptoms. These symptoms include bleeding, pain, skin condition, premenstrual syndrome ( PMS ), among others. The app in return, gives you predictions regarding your period, ovulation, and PMS . The app looks like this: Using the app you can easily see if there are any irregularities in your cycle, and/or see when you should be expecting phases such as ovulation, period, or PMS , and read about menstruation cycle in general if you're interested. Some symptoms can be recorded and predicted directly, such as bleeding, others are less directly observed such as ovulation. The app uses observations from other related symptoms to predict these phases. The data that the app collects is invaluable to a better understanding of the menstruation cycle, related diseases, and even (birth control) pills. I hope the data helps our society to understand cycles better, and potentially improve half of the population's lives. Statice by WATTx Fortunately, people at Clue value users' privacy a great deal. But that means they can't have a hackathon and give people real users' data to work on. Here's where the Statice platform designed by the nice folks at WATTx comes in. Their platform is still taking shape and this hackathon was also a test for them to see how the platform works. But in short, they take the real data, produce some data using the real one, which resembles the real data, but users' profiles from the real data are not revealed by looking at the synthesized data. Their synthesized data was what we worked on, and using that data we developed our method. Then to test the method against the real data, we would submit our method in the form of a docker container, and they would use that container to train our models on the real data, and then test it against the test set, again from the real data. And then, we would see the score of the method, or failed in case there was something wrong with the code or during the run. Goal The goal of the hackathon was to analyze Clue users' data given users' history, and predict users' symptoms for their next cycle. We were supposed to predict the probability of a user tracking a symptom for each day of the next cycle. Out of total 80 symptoms for which we had data, we were to predict only 16, but we could use other symptoms' history and data for the prediction. Method The main part of the data was in the form of user interactions. Basically one row for each (user, symptom, date) tuple. We transformed the data into one row per user as our input, and a vector for each symptom as output, and then used a simple regularized linear model to predict the output, i.e. Lasso. A more detailed explanation of the method as well as the code itself are available on github . Results At the end of the two days, we all submitted our best models to test against a test dataset which was taken out from the data for this purpose, and our team managed to win the prize for the best performing method: Organizers' summary A nice summary of the event was written by Laure Sorba, WATTx UX strategist, available here . And here is the diagram showing her summary of the event, available on the same page:","tags":"machine-learning","url":"clue-wattx-hackathon.html","loc":"clue-wattx-hackathon.html"},{"title":"TV -ad Attribution, Gaussian Processes","text":"Problem description: This work was done in Mister Spex GmbH , and slides of a presentation I gave at PyData meetup are available now here . There is a website, in this case an e-shop, and we have information about user sessions on the website. We also have information about TV -ads shown to the public requested by the company. The question is, which of those sessions on the website are there because of the TV -ads. There are some obstacles to answer the above question: Users don't tell the website why they came to the website, or how they found about the service. Users use different channels to get to the website. They might have seen an ad on TV and then have used google to find the exact address to the website. As a result, we change the question to the following: what is the likelihood of each session to be because of a TV -ad shown recently. Hence, the above input/output is given/desired: Input: One entry for each session which are date-time labeled. A list of date-time labeled events. Output: What is the likelihood of each session to be influenced by a nearby event. Prepare the data We reduce the granularity of the data and aggregate it by minute. As a result, we have session count per minute, and the time of ( TV -ad) events by minute. This results in a time series data having some points on it labeled by our events. One way to see if the events have had an influence on our traffic, is to forget about those events, detect anomalies on our time series data, and see if they've happened around those events. I've used some of those methods in other projects, and they don't make me happy on this data. They either detect wrong signals as anomalies, or miss some not so obvious signals. Before going further, let's look at the data. Fig. 1 illustrates a week worth of data. The X axis represents time (1 for each minute), and the Y axis shows the normalized session count. The data is normalized as X / median(X) simply to anonymize the actual session counts, since the actual numbers are irrelevant to this article. Fig. 1: Normalized session count for a week Some of those sharp peaks are right after a TV -ad event. Field knowledge says most of the traffic caused by a TV -ad comes up to 8 minutes after the event. I take precaution, and assume some error in reported time of the event. Now let's remove any reported session from 2 minutes before the event, to 20 minutes after the event. Fig. 2 and 3 show one day of the data, before and after removing data around reported events. Fig. 2: Normalized session count for a day Fig. 3: Normalized session count for a day, after removing data around reported events Method Intuitively speaking, I would like to remove the parts of the data close to the given events, fit a model to the remaining data, and then see how close the observed data is to the model's predictions. If the observed values are significantly higher than the predicted values, most probably some of those sessions are because of the event. Given an observed and predicted values, deciding whether or not the observed value is significantly larger, can be tricky, and in some cases very arbitrary. Using a method which gives you the probability of an observed value at a given input would make the process much easier. One way of achieving such a goal is to have a method which, intuitively speaking, tells you what it thinks the predicted value is, and also how confident it is. A Gaussian Process ( GP ) is such a model, in a way that it gives a normal distribution at a given input point as the output. This predicted output has a mean and a variance, which we use to see how probable a given observation is, under the assumption that the model correctly describes the data. Given a predicted normal distribution, the further the observed value is from the expected mean, the less expected it is. In other words, the larger the blue area shown in Fig. 4, the more probable it's an outlier. Fig. 4: Normal distribution and three different observed values - credit: thefemalecelebrity.com The area under the curve marked by the blue areas shown in Fig. 4 is between 0 and 1. It is 0 if the expected value is the same as our observed value, and is close to 1 if it is the two observed and expected mean are far from each other. Hereafter score(x) represents this area under the curve. The piece of code bellow shows how this is calculated in python: import math import scipy def phi ( x ): return 0.5 + 0.5 * scipy . special . erf ( x / math . sqrt ( 2 )) def score ( x ): return 1 - abs ( phi ( x ) - phi ( - x )) x_score = score (( x - expected_mean ) / expected_std ) Now for a moment, assume we know a given data point (number of sessions) is influenced by a TV -ad. Still not all those sessions are because of the event. Some of them would have been there even with no TV -ad event. We take the predicted mean as the background session count, and the rest as TV -ad influenced sessions. Therefore, if x is observed value, and x' is predicted value, (x - x') / x is the portion of sessions attributed to a TV -ad event. At the end, we take score(x) * (x - x') / x as the likelihood of each session being attributed to a TV -ad. Implementation and Results I used Python3 , matplotlib to generate the plots, and GPy to train the Gaussian Process model. There are at least two ways of fitting and using GPs in our setting. One is to train a really nice model using the historical data, and use the saved trained model to get predictions for future data. By looking at the data, I would sum these three kernels and optimize on the data: A periodic kernel to handle periodicity A Gaussian ( RBF ) kernel to handle the non-periodic part of the data A white noise kernel to handle fluctuations seen in the data Because there were no big enough server available, training a model on the historical data at once was not feasible. Instead, for each day, I take a small part of the data which includes 10 hours before and after start and end of the day, and train a model on that part of the data. This makes training of the model on a small machine feasible, and it also doesn't require the periodic kernel, which makes the optimization process even faster. Therefore I only use these two kernels: A Gaussian ( RBF ) kernel to handle the non-periodic part of the data A white noise kernel to handle fluctuations seen in the data To summarize, here's what I do: For a given date, take the 44 hours of data corresponding to the given date (10 hours before and 10 hours after the beginning and the end of the day respectively) Remove the part of the data around given any TV -ad events in that range. Fit a GP to the data. Calculate the abovementioned scores for each data point which corresponds to one minute. Using GPy , I train the model as: kernel = GPy.kern.RBF(input_dim=1) + GPy.kern.White(input_dim=1) m = GPy.models.GPRegression(Xtr.reshape(-1,1),ytr.reshape(-1,1),kernel) m.optimize() In the above code, Xtr and ytr are the input/output vectors respectively. The Xtr is a sequence of integers representing the minute distance from where the data started, and ytr is the normalized number of sessions for that minute. This results in a model depicted in Fig. 5. The blue gradient represents the variance of the expected output, showing the expected value should be somewhere in the blue region. Please note that we're only interested in the regions inside the data, and not the region outside the data range. Fig. 5: The trained Gaussian Process model on the 44h of data. Table 1 shows calculated scores and likelihoods of each session in each minute to be influenced by a TV -ad. A negative portion and likelihood simply mean the expected value is larger that the observed, and can safely be ignored. The Is Significant column simply flags if the score is over 0.9 . I would personally attribute only sessions of those minutes to a TV -ad. Also, as expected, not all TV -ad events result in significant rise in the traffic, and some clearly result in more traffic than others. Observed Expected Mean Expected Variance Score Portion Likelihood Is Significant TV -ad 0.96 0.84 0.03 0.52 0.12 0.06 0.96 0.84 0.03 0.52 0.12 0.06 TV 0.83 0.84 0.03 0.06 -0.01 -0.00 0.96 0.84 0.03 0.51 0.12 0.06 0.81 0.84 0.03 0.16 -0.04 -0.01 0.68 0.84 0.03 0.67 -0.24 -0.16 0.85 0.84 0.03 0.04 0.01 0.00 0.72 0.84 0.03 0.52 -0.17 -0.09 0.70 0.84 0.03 0.60 -0.20 -0.12 1.00 0.84 0.03 0.65 0.16 0.10 TV 1.89 0.84 0.03 1.00 0.55 0.55 * 1.19 0.84 0.03 0.96 0.29 0.28 * 0.94 0.85 0.03 0.41 0.10 0.04 0.85 0.85 0.03 0.02 0.01 0.00 0.96 0.85 0.03 0.49 0.12 0.06 0.83 0.85 0.03 0.08 -0.02 -0.00 0.96 0.85 0.03 0.48 0.11 0.06 1.23 0.85 0.03 0.98 0.31 0.31 * 1.11 0.85 0.03 0.87 0.23 0.20 0.85 0.85 0.03 0.01 0.00 0.00 1.04 0.85 0.03 0.74 0.18 0.14 TV 0.77 0.85 0.03 0.38 -0.11 -0.04 1.04 0.85 0.03 0.73 0.18 0.13 1.11 0.85 0.03 0.86 0.23 0.20 1.13 0.85 0.03 0.89 0.24 0.21 1.09 0.86 0.03 0.82 0.21 0.17 1.19 0.86 0.03 0.95 0.28 0.27 * 1.00 0.86 0.03 0.59 0.14 0.08 0.79 0.86 0.03 0.32 -0.09 -0.03 1.11 0.86 0.03 0.84 0.22 0.19 0.64 0.86 0.03 0.80 -0.35 -0.28 0.85 0.86 0.03 0.06 -0.01 -0.00 0.96 0.87 0.03 0.41 0.10 0.04 0.85 0.87 0.03 0.07 -0.02 -0.00 0.83 0.87 0.03 0.17 -0.05 -0.01 1.34 0.87 0.03 0.99 0.35 0.35 * 0.85 0.87 0.03 0.09 -0.02 -0.00 1.17 0.87 0.03 0.91 0.25 0.23 * 0.83 0.88 0.03 0.21 -0.05 -0.01 0.77 0.88 0.03 0.48 -0.14 -0.07 1.02 0.88 0.03 0.59 0.14 0.08 TV 1.62 0.88 0.03 1.00 0.46 0.46 * TV 1.47 0.88 0.03 1.00 0.40 0.40 * TV 1.26 0.89 0.03 0.97 0.29 0.29 * 1.19 0.89 0.03 0.92 0.26 0.23 * 1.28 0.89 0.03 0.97 0.30 0.30 * 0.91 0.89 0.03 0.11 0.03 0.00 1.13 0.89 0.03 0.82 0.21 0.17 1.15 0.90 0.03 0.86 0.22 0.19 TV 4.45 0.90 0.03 1.00 0.80 0.80 * TV 5.40 0.90 0.03 1.00 0.83 0.83 * 3.21 0.90 0.03 1.00 0.72 0.72 * 2.30 0.91 0.03 1.00 0.61 0.61 * 1.96 0.91 0.03 1.00 0.54 0.54 * 1.98 0.91 0.03 1.00 0.54 0.54 * 1.30 0.91 0.03 0.97 0.30 0.29 * 1.47 0.92 0.03 1.00 0.38 0.37 * 0.94 0.92 0.03 0.08 0.02 0.00 1.00 0.92 0.03 0.35 0.08 0.03 1.40 0.93 0.03 0.99 0.34 0.34 * 1.30 0.93 0.03 0.97 0.28 0.28 * 1.11 0.93 0.03 0.70 0.16 0.11 1.00 0.93 0.03 0.30 0.07 0.02 1.19 0.94 0.03 0.87 0.21 0.18 1.53 0.94 0.03 1.00 0.39 0.39 * 1.19 0.94 0.03 0.86 0.21 0.18 1.17 0.95 0.03 0.81 0.19 0.16 1.17 0.95 0.03 0.81 0.19 0.15 1.11 0.95 0.03 0.64 0.14 0.09 TV 1.55 0.96 0.03 1.00 0.38 0.38 * TV 2.43 0.96 0.03 1.00 0.60 0.60 * 1.51 0.96 0.03 1.00 0.36 0.36 * 1.62 0.97 0.03 1.00 0.40 0.40 * 1.26 0.97 0.03 0.91 0.23 0.21 * 1.02 0.97 0.03 0.23 0.05 0.01 1.04 0.97 0.03 0.32 0.06 0.02 1.19 0.98 0.03 0.80 0.18 0.14 1.11 0.98 0.03 0.55 0.11 0.06 0.89 0.98 0.03 0.42 -0.10 -0.04 TV 1.30 0.99 0.03 0.94 0.24 0.22 * 1.13 0.99 0.03 0.59 0.12 0.07 1.19 0.99 0.03 0.77 0.17 0.13 TV 1.28 1.00 0.03 0.91 0.22 0.20 * 1.04 1.00 0.03 0.20 0.04 0.01 1.53 1.00 0.03 1.00 0.34 0.34 * 1.26 1.01 0.03 0.87 0.20 0.17 1.11 1.01 0.03 0.44 0.09 0.04 1.34 1.01 0.03 0.96 0.24 0.23 * 1.26 1.02 0.03 0.86 0.19 0.16 1.04 1.02 0.03 0.11 0.02 0.00 1.21 1.02 0.03 0.76 0.16 0.12 1.17 1.03 0.03 0.63 0.12 0.08 0.81 1.03 0.03 0.83 -0.27 -0.23 1.09 1.03 0.03 0.25 0.05 0.01 1.23 1.04 0.03 0.78 0.16 0.13 1.36 1.04 0.03 0.96 0.24 0.23 * 0.87 1.04 0.03 0.71 -0.20 -0.14 1.09 1.05 0.03 0.19 0.04 0.01 1.00 1.05 0.03 0.24 -0.05 -0.01 1.36 1.05 0.03 0.95 0.23 0.22 * 1.30 1.05 0.03 0.87 0.19 0.16 1.15 1.09 0.02 0.32 0.06 0.02 1.26 1.09 0.02 0.72 0.13 0.10 1.40 1.09 0.02 0.96 0.22 0.21 * TV 2.57 1.09 0.02 1.00 0.58 0.58 * 2.77 1.10 0.02 1.00 0.60 0.60 * TV 1.51 1.10 0.02 0.99 0.27 0.27 * 1.30 1.10 0.02 0.80 0.15 0.12 1.34 1.10 0.02 0.87 0.18 0.16 1.30 1.10 0.02 0.79 0.15 0.12 1.06 1.11 0.02 0.21 -0.04 -0.01 1.30 1.11 0.02 0.78 0.15 0.11 1.19 1.11 0.02 0.40 0.07 0.03 1.23 1.11 0.02 0.56 0.10 0.06 1.19 1.11 0.02 0.38 0.06 0.02 1.40 1.12 0.02 0.94 0.20 0.19 * 1.00 1.12 0.02 0.55 -0.12 -0.07 TV 1.32 1.12 0.02 0.80 0.15 0.12 TV 1.87 1.12 0.02 1.00 0.40 0.40 * 1.62 1.12 0.02 1.00 0.31 0.30 * 1.36 1.13 0.02 0.87 0.17 0.15 1.06 1.13 0.02 0.32 -0.06 -0.02 0.96 1.13 0.02 0.73 -0.18 -0.13 1.72 1.13 0.02 1.00 0.34 0.34 * 1.06 1.13 0.02 0.34 -0.06 -0.02 1.28 1.13 0.02 0.64 0.11 0.07 1.04 1.13 0.02 0.45 -0.09 -0.04 1.28 1.14 0.02 0.63 0.11 0.07 1.34 1.14 0.02 0.81 0.15 0.12 1.26 1.14 0.02 0.55 0.09 0.05 1.21 1.14 0.02 0.36 0.06 0.02 1.26 1.14 0.02 0.54 0.09 0.05 1.40 1.14 0.02 0.91 0.19 0.17 * 1.00 1.14 0.02 0.64 -0.14 -0.09 0.98 1.14 0.02 0.71 -0.17 -0.12 Table 1: A piece of output of the method","tags":"machine-learning","url":"tv-ad-attribution-gaussian-processes.html","loc":"tv-ad-attribution-gaussian-processes.html"},{"title":"On the ethics of CRISPR","text":"I was reading this article on the ethics of editing human genome and I realized there's a missing point in there. CRISPR in short is a technology that allows us to edit our own genome. Of course it has countless number of useful applications as it's very simply depicted in the above picture (credit: economist). But recently Chinese scientists have genetically modified human embryos ( link ). Fortunately there's been some discussion going on in the community about the ethics of editing human embryo's genome, or in general human genome. The question I want people to think about is: who should/will have access to this technology? Obviously this technology will be very expensive when it reaches the market. Does it mean only rich people will be able to use it? And I'm not talking about the applications which concern diseases; I'm rather talking about editing your child's genome, to make it smarter, or stronger, or whatever you feel like it. We already have too much of a difference between classes in our society. This difference has been empowered partially, if not mostly, by the fact that people have been able to accumulate wealth through time and even generations. If we allow them to be the ones benefiting a technology like CRISPR , we will practically give them the power to annihilate the lower classes through time. One piece of good news is that some people are talking about how to regulate this sort of technology ( link ). I believe we should start a conversation about who should have access to the technology. This concern is regardless of how and what we decide about the ethics of the method for different applications. If we, as human species, decide not to use it at all, then nobody has access to it. But if we give it a go, then we should make sure all classes of the society have equal access to it. The least and the last we need is to have a rich sub-population with higher physical and mental capabilities. EDIT : Thanks to Ian Sample, found a podcast by the guardian here .","tags":"ethics","url":"on-the-ethics-of-crispr.html","loc":"on-the-ethics-of-crispr.html"},{"title":"Zimbra Auto Provisioning from FreeIPA","text":"After quite a few days struggling to configure a Zimbra server so that it automatically fetches users from our freeIPA ( LDAP ) server, I finally managed to have a configuration which works. I got help from a bunch of pages like this and this one. This comes after you fix the external LDAP authentication and probably also external GAL configuration on your Zimbra server. zmprov gives you a nice terminal to configure the server: $ su - zimbra $ zmprov This is the set of commands I used to set it up: prov&gt; md mysampledomain.net zimbraAutoProvAccountNameMap \"uid\" prov&gt; md mysampledomain.net zimbraAutoProvAttrMap \"givenName=givenName\" prov&gt; md mysampledomain.net +zimbraAutoProvAttrMap \"sn=sn\" prov&gt; md mysampledomain.net zimbraAutoProvBatchSize 80 prov&gt; md mysampledomain.net zimbraAutoProvLdapAdminBindDn \"uid=mail_server,cn=users,cn=accounts,dc=mysampledomain,dc=net\" prov&gt; md mysampledomain.net zimbraAutoProvLdapAdminBindPassword \"myverysecretpassword\" prov&gt; md mysampledomain.net zimbraAutoProvLdapBindDn \"uid=mail_server,cn=users,cn=accounts,dc=mysampledomain,dc=net\" prov&gt; md mysampledomain.net zimbraAutoProvLdapSearchBase \"cn=accounts,dc=mysampledomain,dc=net\" prov&gt; md mysampledomain.net zimbraAutoProvLdapSearchFilter \"(&amp;(ObjectClass=person))\" prov&gt; md mysampledomain.net zimbraAutoProvLdapStartTlsEnabled TRUE prov&gt; md mysampledomain.net zimbraAutoProvLdapURL \"ldaps://ipa.mysampledomain.net:636\" prov&gt; md mysampledomain.net zimbraAutoProvPollingInterval \"10m\" prov&gt; md mysampledomain.net zimbraAutoProvScheduledDomains \"mysampledomain.net\" prov&gt; md mysampledomain.net zimbraAutoProvMode \"EAGER\" To diagnose why the system wasn't working, I also had to figure out where the log files are, and how to produce more logs. Oddly enough, they're not in /var/log , and instead they are written by default in /opt/zimbra/log/mailbox.log , or other related files in that folder. I added log4j.logger.zimbra.autoprov=TRACE at the end of my /opt/zimbra/conf/log4j.properties file, which will be overwritten next time I restart the services using the configurations in /opt/zimbra/conf/log4j.properties.in . Finally to make the logging system reload the logging configuration, you need to run zmprov rlog . You find more info here .","tags":"sysadmin","url":"zimbra-auto-provisioning-from-freeipa.html","loc":"zimbra-auto-provisioning-from-freeipa.html"},{"title":"synapse.org","text":"I decided to actually write on this blog (decision was made this morning and is final :D ). On June 2nd I received an email from my adviser telling me about the DREAM Challenges . I kind of liked it as some sub-challenges fit exactly what I've been doing for that past few months, and that project is already in a phase that we've submitted a manuscript about it. So why not? Going through their pages, I realized they've got a python client to interact with their databases; that's cool, but it didn't support python 3. As a result I made this pull request and at the moment of writing this post, they seem to be interested in accepting the commits (although the changes are not perfect and/or complete and more work is required). Later I was reading challenge descriptions which made me interact with the website for a while, and that made me a bit frustrated. I have to say, I'm not a web-developer and I'm talking as a simple end user. If I was the web-site's development manager, I'd add these to the first possible sprint : Add a proper navigation system to the website; currently you can only easily navigate within a challenge/project. You need to go to the home page first to go anywhere else. Try to go to DREAM challenges, you need to go to the home page, wait for that timer based frame to come which has the link to the challenge, click on it. In my browser (chromium), I have to refresh the whole page if I miss it once, or wait for it to come again. So: add proper links, menus, or whatever you think suits the framework for a nice navigation to projects. Within a challenge description page, click on a link and the whole page will be reloaded, slowly and painfully. For some reason the layout of the page changes through time after each part of the page is loaded. A simple markdown based static website works much faster and better, if you want it to be fancy, add proper AJAX components/codes to make it smooth and fast. The website has serious performance issues and seems unnecessarily heavy. I'm pretty sure developers have their reason for having the website the way it is, but those reasons must be justified/changed.","tags":"blog","url":"synapseorg.html","loc":"synapseorg.html"}]};